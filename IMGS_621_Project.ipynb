{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f5322-a227-4a19-b82c-dda97e471b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def create_labeled_dataset(base_path, output_path, num_subjects=27, ignore_subjects=[1, 2, 3],\n",
    "                           test_size=0.2, valid_size=0.1, random_state=42, n_workers=4):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    csv_headers = {\n",
    "        'index': 'uint32',\n",
    "        'horiz_coord': 'uint16',\n",
    "        'vert_coord': 'uint16',\n",
    "        'gaze_type': 'str',\n",
    "        'timestamp_microseconds': 'uint64',\n",
    "        'center0': 'float32',\n",
    "        'center1': 'float32',\n",
    "        'large_pupil_movement': 'bool',\n",
    "        'fname': 'str'\n",
    "    }\n",
    "\n",
    "    subjects = [i for i in range(1, num_subjects + 1) if i not in ignore_subjects]\n",
    "    eye_ids = [0, 1]\n",
    "\n",
    "    print(f\"Processing subjects: {subjects}\")\n",
    "    print(f\"Ignoring subjects: {ignore_subjects}\")\n",
    "\n",
    "    subject_eye_combinations = [(subject, eye_id) for subject in subjects for eye_id in eye_ids]\n",
    "\n",
    "    print(\"Loading CSV files\")\n",
    "    dataframes = []\n",
    "\n",
    "    for subject, eye_id in subject_eye_combinations:\n",
    "        csv_path = os.path.join(base_path, f'user{subject}/data_{eye_id}.csv')\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"Warning: {csv_path} does not exist, skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, names=list(csv_headers.keys()), dtype=csv_headers)\n",
    "            df['subject'] = subject\n",
    "            df['eye_id'] = eye_id\n",
    "            dataframes.append(df)\n",
    "            print(f\"Loaded data for user {subject}, eye {eye_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {csv_path}: {e}\")\n",
    "\n",
    "    if not dataframes:\n",
    "        raise ValueError(\"No data was loaded. Check paths and file formats.\")\n",
    "\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    gaze_types = combined_df['gaze_type'].unique()\n",
    "    print(f\"Found {len(gaze_types)} unique gaze types: {gaze_types}\")\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "    available_subjects = np.unique(combined_df['subject'])\n",
    "\n",
    "    num_valid_subjects = max(1, int(len(available_subjects) * valid_size))\n",
    "    num_test_subjects = max(1, int(len(available_subjects) * test_size))\n",
    "\n",
    "    valid_subjects = np.random.choice(available_subjects, size=num_valid_subjects, replace=False)\n",
    "    remaining_subjects = np.setdiff1d(available_subjects, valid_subjects)\n",
    "    test_subjects = np.random.choice(remaining_subjects, size=num_test_subjects, replace=False)\n",
    "    train_subjects = np.setdiff1d(remaining_subjects, test_subjects)\n",
    "\n",
    "    print(f\"Train subjects: {train_subjects}\")\n",
    "    print(f\"Validation subjects: {valid_subjects}\")\n",
    "    print(f\"Test subjects: {test_subjects}\")\n",
    "\n",
    "    train_df = combined_df[combined_df['subject'].isin(train_subjects)]\n",
    "    valid_df = combined_df[combined_df['subject'].isin(valid_subjects)]\n",
    "    test_df = combined_df[combined_df['subject'].isin(test_subjects)]\n",
    "\n",
    "    print(f\"Train set: {len(train_df)} samples\")\n",
    "    print(f\"Validation set: {len(valid_df)} samples\")\n",
    "    print(f\"Test set: {len(test_df)} samples\")\n",
    "\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        split_dirs = [os.path.join(output_path, split, gaze_type) for gaze_type in gaze_types]\n",
    "        for directory in split_dirs:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    def copy_single_image(row, split_name):\n",
    "        subject, eye_id, fname, gaze_type = row['subject'], row['eye_id'], row['fname'], row['gaze_type']\n",
    "        src_path = os.path.join(base_path, f'user{subject}/{eye_id}/frames/{fname}')\n",
    "        filename, ext = os.path.splitext(fname)\n",
    "        new_fname = f\"user{subject}_eye{eye_id}_{filename}{ext}\"\n",
    "        dst_path = os.path.join(output_path, split_name, gaze_type, new_fname)\n",
    "\n",
    "        if os.path.exists(src_path):\n",
    "            try:\n",
    "                shutil.copy2(src_path, dst_path)\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"Error copying {src_path} to {dst_path}: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"Warning: Source image {src_path} does not exist\")\n",
    "            return False\n",
    "\n",
    "    def process_dataframe(df, split_name):\n",
    "        print(f\"Processing {split_name} set with {len(df)} images...\")\n",
    "        copy_func = partial(copy_single_image, split_name=split_name)\n",
    "        with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "            records = df.to_dict('records')\n",
    "            results = list(tqdm(executor.map(copy_func, records), total=len(records), desc=f\"{split_name} set\"))\n",
    "        successes = sum(results)\n",
    "        failures = len(results) - successes\n",
    "        return successes, failures\n",
    "\n",
    "    print(\"Organizing images by split and gaze type...\")\n",
    "    train_count, train_errors = process_dataframe(train_df, 'train')\n",
    "    valid_count, valid_errors = process_dataframe(valid_df, 'valid')\n",
    "    test_count, test_errors = process_dataframe(test_df, 'test')\n",
    "\n",
    "    print(\"\\nDataset organization complete!\")\n",
    "    print(f\"Train set: {train_count} images copied, {train_errors} errors\")\n",
    "    print(f\"Validation set: {valid_count} images copied, {valid_errors} errors\")\n",
    "    print(f\"Test set: {test_count} images copied, {test_errors} errors\")\n",
    "\n",
    "    train_df.to_csv(os.path.join(output_path, 'train_data.csv'), index=False)\n",
    "    valid_df.to_csv(os.path.join(output_path, 'valid_data.csv'), index=False)\n",
    "    test_df.to_csv(os.path.join(output_path, 'test_data.csv'), index=False)\n",
    "    combined_df.to_csv(os.path.join(output_path, 'all_data.csv'), index=False)\n",
    "\n",
    "    print(\"\\nSaved split data to CSV files\")\n",
    "\n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    for split_name, df in [('Train', train_df), ('Validation', valid_df), ('Test', test_df)]:\n",
    "        print(f\"\\n{split_name} Set:\")\n",
    "        stats = df['gaze_type'].value_counts()\n",
    "        for gaze_type, count in stats.items():\n",
    "            print(f\"  {gaze_type}: {count} images\")\n",
    "\n",
    "\n",
    "base_path = \"D:/Triya Belani/angelopoulos_eye_data\"\n",
    "output_path = \"D:/Triya Belani/Output\"\n",
    "\n",
    "\n",
    "create_labeled_dataset(\n",
    "    base_path,\n",
    "    output_path,\n",
    "    ignore_subjects=[1, 2, 3],\n",
    "    test_size=0.2,\n",
    "    valid_size=0.1,\n",
    "    n_workers=2  \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb695a27-79a3-4658-a160-85bab7986656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "\n",
    "DATA_PATH = r\"D:\\Triya Belani\\Output\"  \n",
    "\n",
    "class GazeDataset(Dataset):\n",
    "    \"\"\"Dataset for eye gaze classification\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        self.root_dir = os.path.join(root_dir, split)\n",
    "        self.transform = transform\n",
    "        print(f\"[INFO] Initializing GazeDataset for split: {split} at {self.root_dir}\")\n",
    "\n",
    "        if not os.path.exists(self.root_dir):\n",
    "            print(f\"[ERROR] Directory not found: {self.root_dir}\")\n",
    "            raise FileNotFoundError(f\"Directory not found: {self.root_dir}\")\n",
    "\n",
    "        # Get all class directories\n",
    "        self.classes = [d for d in sorted(os.listdir(self.root_dir))\n",
    "                        if os.path.isdir(os.path.join(self.root_dir, d))]\n",
    "        print(f\"[DEBUG] Classes found: {self.classes}\")\n",
    "\n",
    "        if not self.classes:\n",
    "            print(f\"[ERROR] No class directories found in {self.root_dir}\")\n",
    "            raise ValueError(f\"No class directories found in {self.root_dir}\")\n",
    "\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        print(f\"[DEBUG] Class to index mapping: {self.class_to_idx}\")\n",
    "\n",
    "        # Get all image paths and labels\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(self.root_dir, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    self.image_paths.append(os.path.join(class_dir, img_name))\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "        print(f\"[INFO] Number of images found: {len(self.image_paths)} for split: {split}\")\n",
    "\n",
    "        if not self.image_paths:\n",
    "            print(f\"[ERROR] No images found in {self.root_dir}\")\n",
    "            raise ValueError(f\"No images found in {self.root_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            label = self.labels[idx]\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Error loading image {img_path}: {e}\")\n",
    "            # Return a placeholder in case of error\n",
    "            placeholder = torch.zeros((3, 64, 64)) if self.transform else Image.new('RGB', (64, 64))\n",
    "            return placeholder, self.labels[idx]\n",
    "\n",
    "# Define optimized transformations - reduced image size for faster processing\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),  # Add some data augmentation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class LightweightGazeClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(LightweightGazeClassifier, self).__init__()\n",
    "        print(f\"[INFO] Initializing LightweightGazeClassifier with {num_classes} classes\")\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 8 * 8, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"[DEBUG] Input shape: {x.shape}\")\n",
    "        x = self.features(x)\n",
    "        print(f\"[DEBUG] Shape after features: {x.shape}\")\n",
    "        x = torch.flatten(x, 1)\n",
    "        print(f\"[DEBUG] Shape after flatten: {x.shape}\")\n",
    "        x = self.classifier(x)\n",
    "        print(f\"[DEBUG] Output shape: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "def load_datasets():\n",
    "    try:\n",
    "        print(\"[INFO] Loading datasets...\")\n",
    "        train_dataset = GazeDataset(root_dir=DATA_PATH, split='train', transform=train_transforms)\n",
    "        valid_dataset = GazeDataset(root_dir=DATA_PATH, split='valid', transform=test_transforms)\n",
    "        test_dataset = GazeDataset(root_dir=DATA_PATH, split='test', transform=test_transforms)\n",
    "\n",
    "        # Use more workers for CPU-bound operations and smaller batch size for memory efficiency\n",
    "        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0, pin_memory=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=0, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "        print(f\"[INFO] Number of training samples: {len(train_dataset)}\")\n",
    "        print(f\"[INFO] Number of validation samples: {len(valid_dataset)}\")\n",
    "        print(f\"[INFO] Number of test samples: {len(test_dataset)}\")\n",
    "        print(f\"[INFO] Number of classes: {len(train_dataset.classes)}\")\n",
    "        print(f\"[INFO] Classes: {train_dataset.classes}\")\n",
    "\n",
    "        return train_dataset, valid_dataset, test_dataset, train_loader, valid_loader, test_loader\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error loading datasets: {e}\")\n",
    "        raise\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    # validate only every other epoch\n",
    "    validate_every = 2\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        print(f\"[INFO] Starting epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loop):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Limit debug output to save time\n",
    "            if batch_idx < 1:\n",
    "                print(f\"[DEBUG] Batch {batch_idx}: Loss={loss.item():.4f}, Predicted={predicted[:3].cpu().numpy()}, Labels={labels[:3].cpu().numpy()}\")\n",
    "\n",
    "            train_loop.set_postfix(loss=f\"{loss.item():.4f}\", accuracy=f\"{100 * correct / total:.2f}%\")\n",
    "\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_train_acc = 100 * correct / total\n",
    "\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['train_acc'].append(epoch_train_acc)\n",
    "\n",
    "        # only run every validate_every epochs or on the last epoch\n",
    "        do_validate = (epoch % validate_every == 0) or (epoch == num_epochs - 1)\n",
    "\n",
    "        if do_validate:\n",
    "            model.eval()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, labels) in enumerate(valid_loader):\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "\n",
    "                    if batch_idx < 1:\n",
    "                        print(f\"[DEBUG] [Validation] Batch {batch_idx}: Loss={loss.item():.4f}, Predicted={predicted[:3].cpu().numpy()}, Labels={labels[:3].cpu().numpy()}\")\n",
    "\n",
    "            epoch_val_loss = running_loss / len(valid_loader.dataset)\n",
    "            epoch_val_acc = 100 * correct / total\n",
    "\n",
    "            history['val_loss'].append(epoch_val_loss)\n",
    "            history['val_acc'].append(epoch_val_acc)\n",
    "\n",
    "            if epoch_val_acc > best_val_acc:\n",
    "                best_val_acc = epoch_val_acc\n",
    "                torch.save(model.state_dict(), os.path.join(DATA_PATH, 'best_model.pth'))\n",
    "                print(f\"[INFO] Model saved with validation accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "            print(f\"[INFO] Epoch {epoch+1}/{num_epochs} - \"\n",
    "                f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}% - \"\n",
    "                f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}% - \")\n",
    "        else:\n",
    "            # If not validating, append previous values to keep history aligned\n",
    "            if history['val_loss']:\n",
    "                history['val_loss'].append(history['val_loss'][-1])\n",
    "                history['val_acc'].append(history['val_acc'][-1])\n",
    "            else:\n",
    "                history['val_loss'].append(0)\n",
    "                history['val_acc'].append(0)\n",
    "\n",
    "            print(f\"[INFO] Epoch {epoch+1}/{num_epochs} - \"\n",
    "                f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}% - \"\n",
    "                f\"Skipping validation\")\n",
    "\n",
    "        scheduler.step()\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"[INFO] Epoch time: {epoch_time:.1f}s\")\n",
    "\n",
    "    return history\n",
    "\n",
    "def test_model(model, test_loader, criterion, classes):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_loop = tqdm(test_loader, desc=\"Testing\")\n",
    "        for batch_idx, (inputs, labels) in enumerate(test_loop):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            if batch_idx < 1:\n",
    "                print(f\"[DEBUG] [Testing] Batch {batch_idx}: Loss={loss.item():.4f}, Predicted={predicted[:3].cpu().numpy()}, Labels={labels[:3].cpu().numpy()}\")\n",
    "\n",
    "            test_loop.set_postfix(loss=f\"{loss.item():.4f}\", accuracy=f\"{100 * correct / total:.2f}%\")\n",
    "\n",
    "    test_loss = running_loss / len(test_loader.dataset)\n",
    "    test_acc = 100 * correct / total\n",
    "\n",
    "    print(f\"[RESULT] Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(DATA_PATH, 'confusion_matrix.png'))\n",
    "\n",
    "    print(\"\\n[RESULT] Classification Report:\")\n",
    "    print(classification_report(all_labels, all_predictions, target_names=classes))\n",
    "\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(DATA_PATH, 'training_history.png'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        print(\"[INFO] Starting main execution...\")\n",
    "        train_dataset, valid_dataset, test_dataset, train_loader, valid_loader, test_loader = load_datasets()\n",
    "        num_classes = len(train_dataset.classes)\n",
    "        print(f\"[INFO] Number of classes for model: {num_classes}\")\n",
    "        model = LightweightGazeClassifier(num_classes=num_classes)\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Use more efficient criterion and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)#, verbose=True)\n",
    "\n",
    "        print(\"[INFO] Starting training...\")\n",
    "\n",
    "        history = train_model(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                valid_loader=valid_loader,\n",
    "                criterion=criterion,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                num_epochs=5)\n",
    "\n",
    "        print(\"[INFO] Training complete. Plotting history...\")\n",
    "        plot_history(history)\n",
    "\n",
    "        best_model_path = os.path.join(DATA_PATH, 'best_model.pth')\n",
    "        if os.path.exists(best_model_path):\n",
    "            model.load_state_dict(torch.load(best_model_path))\n",
    "            print(\"[INFO] Loaded best model for testing\")\n",
    "\n",
    "        print(\"[INFO] Starting testing...\")\n",
    "        test_loss, test_acc = test_model(model, test_loader, criterion, classes=test_dataset.classes)\n",
    "        print(f\"[RESULT] Final test accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     start_time = time.time()\n",
    "#     main()\n",
    "#     total_time = time.time() - start_time\n",
    "#     print(f\"[INFO] Total execution time: {total_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    train_dataset, valid_dataset, _, _, _, _ = load_datasets()\n",
    "    \n",
    "    # Prepare quick training subsets and data loaders\n",
    "    quick_train_dataset = torch.utils.data.Subset(train_dataset, range(min(100, len(train_dataset))))\n",
    "    quick_valid_dataset = torch.utils.data.Subset(valid_dataset, range(min(50, len(valid_dataset))))\n",
    "    quick_train_loader = DataLoader(quick_train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "    quick_valid_loader = DataLoader(quick_valid_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Quick test training setup\n",
    "    print(\"[INFO] Starting quick test training...\")\n",
    "    num_classes = len(train_dataset.classes) \n",
    "    model = LightweightGazeClassifier(num_classes=num_classes).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "    \n",
    "    # Run training for 2 epochs\n",
    "    train_model(model, quick_train_loader, quick_valid_loader, criterion, optimizer, scheduler, num_epochs=2)\n",
    "    print(\"[INFO] Quick test completed successfully!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
